{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzZzR2-U43r4"
   },
   "source": [
    "# Joint Intent Classification and Slot filling with BERT\n",
    "This notebook is based on the paper __BERT for Joint Intent Classification and Slot Filling__ by Chen et al. (2019), https://arxiv.org/abs/1902.10909 but on a different dataset made for a class project.\n",
    "\n",
    "Ideas were also taken from https://github.com/monologg/JointBERT, which is a PyTorch implementation of the paper with the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjRqJ7sS5vsQ"
   },
   "source": [
    "## Install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OO2HRXgV5HA",
    "outputId": "c783f199-c246-4b52-e5ff-c650f3e03019",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.12.0 regex-2022.10.31 tokenizers-0.13.2 transformers-4.26.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbLDTz7m5_rs"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj_x96qa6C20",
    "outputId": "6a8fc8ee-90f2-470b-b65a-ecb22b253b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-29 16:53:59--  https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/train.json\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/train.json [following]\n",
      "--2021-01-29 16:53:59--  https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/train.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5055766 (4.8M) [text/plain]\n",
      "Saving to: ‘train.json.3’\n",
      "\n",
      "train.json.3        100%[===================>]   4.82M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2021-01-29 16:54:00 (95.0 MB/s) - ‘train.json.3’ saved [5055766/5055766]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-T_1uc46O6s",
    "outputId": "3b2154ec-0efa-46bd-8f72-b3c25746fd63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-29 16:54:00--  https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/dev.json\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/dev.json [following]\n",
      "--2021-01-29 16:54:00--  https://raw.githubusercontent.com/ShawonAshraf/nlu-jointbert-dl2021/main/data/nlu_traindev/dev.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248459 (243K) [text/plain]\n",
      "Saving to: ‘dev.json.3’\n",
      "\n",
      "dev.json.3          100%[===================>] 242.64K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2021-01-29 16:54:00 (42.2 MB/s) - ‘dev.json.3’ saved [248459/248459]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ShawonAshraf/nlu-jointbert-dl2021/raw/main/data/nlu_traindev/dev.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpJIziCh6oy8"
   },
   "source": [
    "## Read data from json files\n",
    "\n",
    "Data is of the following format\n",
    "````json5\n",
    "{\n",
    "  \"text\": \"\",\n",
    "  \"positions\": [{}],\n",
    "  \"slots\": [{}],\n",
    "  \"intent\": \"\"\n",
    "}\n",
    "````\n",
    "\n",
    "We will be using `text` as the input and `slots` and `intent` as lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqYLzmWhVT84"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class RawData(object):\n",
    "    def __init__(self, id, intent, positions, slots, text):\n",
    "        self.id = id\n",
    "        self.intent = intent\n",
    "        self.positions = positions\n",
    "        self.slots = slots\n",
    "        self.text = text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(json.dumps(self.__dict__, indent=2))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "reads json from data file\n",
    "returns a list containing DataInstance objects\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_train_json_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        intents = []\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "            for k in data.keys():\n",
    "                intent = data[k][\"intent\"]\n",
    "                positions = data[k][\"positions\"]\n",
    "                slots = data[k][\"slots\"]\n",
    "                text = data[k][\"text\"]\n",
    "\n",
    "                temp = RawData(k, intent, positions, slots, text)\n",
    "                intents.append(temp)\n",
    "\n",
    "        return intents\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No file found with that path!\")\n",
    "\n",
    "# read from json file\n",
    "train_data = read_train_json_file(\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgnD-4G3VT84",
    "outputId": "b97b2f95-64b3-4aff-9fd7-7586467a2dc2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": \"0\",\n",
       "  \"intent\": \"AddToPlaylist\",\n",
       "  \"positions\": {\n",
       "    \"music_item\": [\n",
       "      6,\n",
       "      9\n",
       "    ],\n",
       "    \"playlist_owner\": [\n",
       "      14,\n",
       "      15\n",
       "    ],\n",
       "    \"playlist\": [\n",
       "      17,\n",
       "      32\n",
       "    ]\n",
       "  },\n",
       "  \"slots\": {\n",
       "    \"music_item\": \"tune\",\n",
       "    \"playlist_owner\": \"my\",\n",
       "    \"playlist\": \"elrow Guest List\"\n",
       "  },\n",
       "  \"text\": \"Add a tune to my elrow Guest List\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_data[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_VahZ-O7JXU"
   },
   "source": [
    "## Load Tokenizer from transformers\n",
    "\n",
    "We will use a pretrained bert model `bert-base-cased` for both Tokenizer and our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA8sh4pLVT84"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQFs80Rg7jj4"
   },
   "source": [
    "# Encode texts from the dataset\n",
    "\n",
    "We have to encode the texts using the tokenizer to create tensors for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpUnhqA2VT84",
    "outputId": "6875034f-9c9d-45f1-85df-683ff83bf6f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/preprocessing.html\n",
    "\n",
    "def encode_texts(tokenizer, texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "texts = [d.text for d in train_data]\n",
    "tds = encode_texts(tokenizer, texts)\n",
    "tds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjaIbB7vVT84"
   },
   "outputs": [],
   "source": [
    "encoded_texts = tds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8iBMHa577iQ"
   },
   "source": [
    "## Encode labels\n",
    "### Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1eTfXPQVT84",
    "outputId": "16ec4896-0d71-40b8-e724-99909199c166",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AddToPlaylist',\n",
       " 'BookRestaurant',\n",
       " 'GetWeather',\n",
       " 'RateBook',\n",
       " 'PlayMusic',\n",
       " 'SearchScreeningEvent',\n",
       " 'SearchCreativeWork']"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "intents = [d.intent for d in train_data]\n",
    "intent_names = list(set(intents))\n",
    "intent_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDCv98XHVT84",
    "outputId": "3edcd2f8-0092-4c9b-9750-2a157dc7b90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AddToPlaylist': 0,\n",
       " 'BookRestaurant': 1,\n",
       " 'GetWeather': 2,\n",
       " 'PlayMusic': 4,\n",
       " 'RateBook': 3,\n",
       " 'SearchCreativeWork': 6,\n",
       " 'SearchScreeningEvent': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_map = dict() # index -> intent\n",
    "for idx, ui in enumerate(intent_names):\n",
    "    intent_map[ui] = idx\n",
    "intent_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-PxXNWcVT84"
   },
   "outputs": [],
   "source": [
    "# map to train_data values\n",
    "def encode_intents(intents, intent_map):\n",
    "    encoded = []\n",
    "    for i in intents:\n",
    "        encoded.append(intent_map[i])\n",
    "    # convert to tf tensor\n",
    "    return tf.convert_to_tensor(encoded, dtype=\"int32\")\n",
    "\n",
    "encoded_intents = encode_intents(intents, intent_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j0i00AW8gjA"
   },
   "source": [
    "### Slots\n",
    "\n",
    "To padd all the texts to the same length, the tokenizer will use special characters. To handle those we need to add <PAD> to slots_names. It can be some other symbol as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNW91LSgVT84",
    "outputId": "2bf96f17-a2f2-45b0-89e9-0111a10ad8fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " 'spatial_relation',\n",
       " 'music_item',\n",
       " 'object_name',\n",
       " 'geographic_poi',\n",
       " 'service',\n",
       " 'artist',\n",
       " 'playlist',\n",
       " 'object_part_of_series_type',\n",
       " 'playlist_owner',\n",
       " 'sort',\n",
       " 'cuisine',\n",
       " 'state',\n",
       " 'year',\n",
       " 'rating_unit',\n",
       " 'location_name',\n",
       " 'restaurant_name',\n",
       " 'object_type',\n",
       " 'country',\n",
       " 'object_select',\n",
       " 'timeRange',\n",
       " 'album',\n",
       " 'entity_name',\n",
       " 'movie_type',\n",
       " 'served_dish',\n",
       " 'city',\n",
       " 'poi',\n",
       " 'movie_name',\n",
       " 'party_size_number',\n",
       " 'genre',\n",
       " 'party_size_description',\n",
       " 'restaurant_type',\n",
       " 'object_location_type',\n",
       " 'best_rating',\n",
       " 'track',\n",
       " 'condition_description',\n",
       " 'rating_value',\n",
       " 'facility',\n",
       " 'current_location',\n",
       " 'condition_temperature']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode slots\n",
    "slot_names = set()\n",
    "for td in train_data:\n",
    "    slots = td.slots\n",
    "    for slot in slots:\n",
    "        slot_names.add(slot)\n",
    "slot_names = list(slot_names)\n",
    "slot_names.insert(0, \"<PAD>\")\n",
    "slot_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Fv7aVEoVT84",
    "outputId": "80e71fc8-08e5-4602-ce97-32f81a62366e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'album': 21,\n",
       " 'artist': 6,\n",
       " 'best_rating': 33,\n",
       " 'city': 25,\n",
       " 'condition_description': 35,\n",
       " 'condition_temperature': 39,\n",
       " 'country': 18,\n",
       " 'cuisine': 11,\n",
       " 'current_location': 38,\n",
       " 'entity_name': 22,\n",
       " 'facility': 37,\n",
       " 'genre': 29,\n",
       " 'geographic_poi': 4,\n",
       " 'location_name': 15,\n",
       " 'movie_name': 27,\n",
       " 'movie_type': 23,\n",
       " 'music_item': 2,\n",
       " 'object_location_type': 32,\n",
       " 'object_name': 3,\n",
       " 'object_part_of_series_type': 8,\n",
       " 'object_select': 19,\n",
       " 'object_type': 17,\n",
       " 'party_size_description': 30,\n",
       " 'party_size_number': 28,\n",
       " 'playlist': 7,\n",
       " 'playlist_owner': 9,\n",
       " 'poi': 26,\n",
       " 'rating_unit': 14,\n",
       " 'rating_value': 36,\n",
       " 'restaurant_name': 16,\n",
       " 'restaurant_type': 31,\n",
       " 'served_dish': 24,\n",
       " 'service': 5,\n",
       " 'sort': 10,\n",
       " 'spatial_relation': 1,\n",
       " 'state': 12,\n",
       " 'timeRange': 20,\n",
       " 'track': 34,\n",
       " 'year': 13}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_map = dict() # slot -> index\n",
    "for idx, us in enumerate(slot_names):\n",
    "    slot_map[us] = idx\n",
    "slot_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62lsuEZoVT84",
    "outputId": "7d5b610d-64b1-4c16-ce8a-e578b8887be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add a tune to my elrow Guest List\n",
      "{'music_item': 'tune', 'playlist_owner': 'my', 'playlist': 'elrow Guest List'}\n",
      "slot_name for my is :  playlist_owner\n"
     ]
    }
   ],
   "source": [
    "# gets slot name from its values\n",
    "def get_slot_from_word(word, slot_dict):\n",
    "    for slot_label,value in slot_dict.items():\n",
    "        if word in value.split():\n",
    "            return slot_label\n",
    "    return None\n",
    "\n",
    "print(train_data[0].text)\n",
    "print(train_data[0].slots)\n",
    "print(\"slot_name for my is : \", get_slot_from_word(\"my\", train_data[0].slots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VUnAC8-VT84"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# find the max encoded test length\n",
    "# tokenizer pads all texts to same length anyway so\n",
    "# just get the length of the first one's input_ids\n",
    "max_len = len(encoded_texts[\"input_ids\"][0])\n",
    "\n",
    "def encode_slots(all_slots, all_texts, \n",
    "                 toknizer, slot_map, max_len=max_len):\n",
    "    encoded_slots = np.zeros(shape=(len(all_texts), max_len), dtype=np.int32)\n",
    "    \n",
    "    for idx, text in enumerate(all_texts):\n",
    "        enc = [] # for this idx, to be added at the end to encoded_slots\n",
    "        \n",
    "        # slot names for this idx\n",
    "        slot_names = all_slots[idx]\n",
    "        \n",
    "        # raw word tokens\n",
    "        # not using bert for this block because bert uses\n",
    "        # a wordpiece tokenizer which will make \n",
    "        # the slot label to word mapping\n",
    "        # difficult\n",
    "        raw_tokens = text.split()\n",
    "\n",
    "        # words or slot_values associated with a certain\n",
    "        # slot_name are contained in the values of the\n",
    "        # dict slots_names\n",
    "        # now this becomes a two way lookup\n",
    "        # first we check if a word belongs to any\n",
    "        # slot label or not and then we add the value from\n",
    "        # slot map to encoded for that word\n",
    "        for rt in raw_tokens:\n",
    "            # use bert tokenizer\n",
    "            # to get wordpiece tokens\n",
    "            bert_tokens = tokenizer.tokenize(rt)\n",
    "            \n",
    "            # find the slot name for a token\n",
    "            rt_slot_name = get_slot_from_word(rt, slot_names)\n",
    "            if rt_slot_name is not None:\n",
    "                # fill with the slot_map value for all ber tokens for rt\n",
    "                enc.append(slot_map[rt_slot_name])\n",
    "                enc.extend([slot_map[rt_slot_name]] * (len(bert_tokens) - 1))\n",
    "\n",
    "            else:\n",
    "                # rt is not associated with any slot name\n",
    "                enc.append(0)\n",
    "\n",
    "        \n",
    "        # now add to encoded_slots\n",
    "        # ignore the first and the last elements\n",
    "        # in encoded text as they're special chars\n",
    "        encoded_slots[idx, 1:len(enc)+1] = enc\n",
    "    \n",
    "    return encoded_slots\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1Hj-St4VT84"
   },
   "outputs": [],
   "source": [
    "all_slots = [td.slots for td in train_data]\n",
    "all_texts = [td.text for td in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIEsMzT8VT84"
   },
   "outputs": [],
   "source": [
    "encoded_slots = encode_slots(all_slots, all_texts, tokenizer, slot_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qw9aA2_gVT84",
    "outputId": "803f542d-6d20-43c5-92ea-8afdcfae224c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 0, 9, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_slots[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Oahd4cP90ms"
   },
   "source": [
    "## Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8zokKeJ-RGI"
   },
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcmMMQaLVT84"
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n",
    "                 model_name=model_name, dropout_prob=0.1):\n",
    "        super().__init__(name=\"joint_intent_slot\")\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.intent_classifier = Dense(intent_num_labels,\n",
    "                                       name=\"intent_classifier\")\n",
    "        self.slot_classifier = Dense(slot_num_labels,\n",
    "                                     name=\"slot_classifier\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # two outputs from BERT\n",
    "        trained_bert = self.bert(inputs, **kwargs)\n",
    "        pooled_output = trained_bert.pooler_output\n",
    "        sequence_output = trained_bert.last_hidden_state\n",
    "        \n",
    "        # sequence_output will be used for slot_filling / classification\n",
    "        sequence_output = self.dropout(sequence_output,\n",
    "                                       training=kwargs.get(\"training\", False))\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        # pooled_output for intent classification\n",
    "        pooled_output = self.dropout(pooled_output,\n",
    "                                     training=kwargs.get(\"training\", False))\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "\n",
    "        return slot_logits, intent_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMpHQUuwVT84",
    "outputId": "0e25cd63-31b1-40a5-8288-8ba9191dc204"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "joint_model = JointIntentAndSlotFillingModel(\n",
    "    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaI_qVxI-Xo4"
   },
   "source": [
    "### Hyperparams, Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMZeZf3JVT84"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "\n",
    "# two outputs, one for slots, another for intents\n",
    "# we have to fine tune for both\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=True),\n",
    "          SparseCategoricalCrossentropy(from_logits=True)]\n",
    "\n",
    "metrics = [SparseCategoricalAccuracy(\"accuracy\")]\n",
    "# compile model\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogJCgyMG-o4A"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uae9vd77VT84",
    "outputId": "c531eae4-781a-4b2d-94a6-0c9a09b567f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f18a67296c8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f18c3b86110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f18a67296c8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f18c3b86110> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f18c1a61b70> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function wrap at 0x7f18c1a61b70> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 126s 294ms/step - loss: 1.3801 - output_1_loss: 0.6952 - output_2_loss: 0.6849 - output_1_accuracy: 0.8540 - output_2_accuracy: 0.7662\n",
      "Epoch 2/2\n",
      "316/316 [==============================] - 93s 294ms/step - loss: 0.1442 - output_1_loss: 0.1101 - output_2_loss: 0.0341 - output_1_accuracy: 0.9680 - output_2_accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "x = {\"input_ids\": encoded_texts[\"input_ids\"], \"token_type_ids\": encoded_texts[\"token_type_ids\"],  \"attention_mask\": encoded_texts[\"attention_mask\"]}\n",
    "\n",
    "history = joint_model.fit(\n",
    "    x, (encoded_slots, encoded_intents), epochs=2, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOSkQ6mZ-sMg"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZqMiC_gZM5s"
   },
   "outputs": [],
   "source": [
    "def nlu(text, tokenizer, model, intent_names, slot_names):\n",
    "    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n",
    "    outputs = model(inputs)\n",
    "    slot_logits, intent_logits = outputs\n",
    "\n",
    "    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, :]\n",
    "    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n",
    "\n",
    "    info = {\"intent\": intent_names[intent_id], \"slots\": {}}\n",
    "\n",
    "    out_dict = {}\n",
    "    # get all slot names and add to out_dict as keys\n",
    "    predicted_slots = set([slot_names[s] for s in slot_ids if s != 0])\n",
    "    for ps in predicted_slots:\n",
    "      out_dict[ps] = []\n",
    "\n",
    "    # check if the text starts with a small letter\n",
    "    if text[0].islower():\n",
    "      tokens = tokenizer.tokenize(text, add_special_tokens=True)\n",
    "    else:\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "        # add all to out_dict\n",
    "        slot_name = slot_names[slot_id]\n",
    "\n",
    "        if slot_name == \"<PAD>\":\n",
    "            continue\n",
    "\n",
    "        # collect tokens\n",
    "        collected_tokens = [token]\n",
    "        idx = tokens.index(token)\n",
    "\n",
    "        # see if it starts with ##\n",
    "        # then it belongs to the previous token\n",
    "        if token.startswith(\"##\"):\n",
    "          # check if the token already exists or not\n",
    "          if tokens[idx - 1] not in out_dict[slot_name]:\n",
    "            collected_tokens.insert(0, tokens[idx - 1])\n",
    "\n",
    "        # add collected tokens to slots\n",
    "        out_dict[slot_name].extend(collected_tokens)\n",
    "\n",
    "    # process out_dict\n",
    "    for slot_name in out_dict:\n",
    "        tokens = out_dict[slot_name]\n",
    "        slot_value = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "        info[\"slots\"][slot_name] = slot_value.strip()\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzuZBVpu_K9Q",
    "outputId": "b9b45e85-192c-432e-8aa7-bf7ab28171a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'AddToPlaylist',\n",
       " 'slots': {'entity_name': 'Madchild', 'playlist': 'Electro Latino'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu(\"add Madchild to Electro Latino\", tokenizer, joint_model, \n",
    "    intent_names, slot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXSXyLFV_fAE",
    "outputId": "70b76afb-a753-4ae0-b0d0-a66178b0787c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'AddToPlaylist',\n",
       " 'slots': {'artist': 'Brian May',\n",
       "  'playlist': 'Reggae Infusions',\n",
       "  'playlist_owner': 'my'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu(\"add Brian May to my Reggae Infusions list\", tokenizer, joint_model, \n",
    "    intent_names, slot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs6x7wmDVT84",
    "outputId": "a2ee8a49-84a1-497a-be22-28b05ffeae94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1611939468"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import calendar\n",
    "import time\n",
    "\n",
    "# to generate timestamps for prediction file\n",
    "def get_time_stamp():\n",
    "    ts = calendar.timegm(time.gmtime())\n",
    "    return ts\n",
    "\n",
    "get_time_stamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWCPwzLP_rQc"
   },
   "source": [
    "## Generate prediction.json\n",
    "\n",
    "This section creates a file containing all the prediction results for inputs from dev.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ikoiObKAYWQ"
   },
   "outputs": [],
   "source": [
    "def read_dev_data(file=\"dev.json\"):\n",
    "    dev_texts = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "        for k in data.keys():\n",
    "          text = data[k][\"text\"]\n",
    "          dev_texts.append(text)\n",
    "          \n",
    "    return dev_texts\n",
    "dev_texts = read_dev_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVAO36SFCTRc",
    "outputId": "7ade69dc-7753-408f-98fd-c16011b4eb1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2887/2887 [02:58<00:00, 16.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(dev_texts))):\n",
    "    res = nlu(dev_texts[i], tokenizer, joint_model, intent_names, slot_names)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85iJGRUIDWEc"
   },
   "outputs": [],
   "source": [
    "# process results\n",
    "results_dict = dict()\n",
    "\n",
    "for idx, res in enumerate(results):\n",
    "    results_dict[str(idx)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blUA-FCxEwbQ"
   },
   "outputs": [],
   "source": [
    "with open(\"prediction.json\", \"w\") as f:\n",
    "    json.dump(results_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRyM8MjiF7vs",
    "outputId": "9862523d-e19d-44b6-ec53-0f4903306f01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"0\": {\n",
      "    \"intent\": \"AddToPlaylist\",\n",
      "    \"slots\": {\n",
      "      \"entity_name\": \"changes & things\",\n",
      "      \"playlist\": \"hot 50\"\n",
      "    }\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"intent\": \"AddToPlaylist\",\n"
     ]
    }
   ],
   "source": [
    "!head prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c889dcc60104b53805760462f8574b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b849da0aef1d4bd0ba71342cc8f04b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffc296d7328472d80178478d604a22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5f6f9b73a44cb198b23a88c3505ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlu_jointbert_dl21.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/tensorflow-2.6-gpu-py38-cu112-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
